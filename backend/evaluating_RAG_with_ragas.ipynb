{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022ec3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluating Agents with RAGAS - AI Makerspace\n",
    "\n",
    "This script demonstrates:\n",
    "1. Setting up RAGAS for agent evaluation\n",
    "2. Creating synthetic test data\n",
    "3. Evaluating RAG chains with different metrics\n",
    "4. Comparing agent performance\n",
    "\n",
    "Based on the AI Makerspace notebook for evaluating RAG systems.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import getpass\n",
    "from uuid import uuid4\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Add backend to path for RAG system integration\n",
    "backend_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'backend'))\n",
    "\n",
    "if backend_path not in sys.path:\n",
    "    sys.path.insert(0, backend_path)\n",
    "\n",
    "# Set up environment variables\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\", \"\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"RAGAS-Evaluation-{uuid4().hex[0:8]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def install_dependencies():\n",
    "    \"\"\"Install required dependencies\"\"\"\n",
    "    print(\"Installing dependencies...\")\n",
    "    # Note: In a real environment, these would be installed via pip\n",
    "    # !pip install -qU ragas==0.2.10\n",
    "    # !pip install -qU langchain-community==0.3.14 langchain-openai==0.2.14\n",
    "    # !pip install -qU unstructured==0.16.12 langgraph==0.2.61 langchain-qdrant==0.2.0\n",
    "    print(\"Dependencies would be installed here\")\n",
    "\n",
    "def setup_ragas_components():\n",
    "    \"\"\"Set up RAGAS components for evaluation using Ollama\"\"\"\n",
    "    from ragas.llms import LangchainLLMWrapper\n",
    "    from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "    from langchain_community.llms import Ollama\n",
    "    from langchain_ollama import OllamaEmbeddings\n",
    "    \n",
    "    # Use Ollama models from your current project - same as RAG system\n",
    "    #generator_llm = LangchainLLMWrapper(Ollama(model=\"qwen:latest\"))\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
    "    generator_embeddings = LangchainEmbeddingsWrapper(OllamaEmbeddings(model=\"nomic-embed-text:latest\"))\n",
    "    \n",
    "    print(\"✅ RAGAS components configured to use same models as RAG system\")\n",
    "    print(f\"   - LLM: qwen:latest\")\n",
    "    print(f\"   - Embeddings: nomic-embed-text:latest\")\n",
    "    \n",
    "    return generator_llm, generator_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8487d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents():\n",
    "    \"\"\"Load documents using the existing RAG system\"\"\"\n",
    "    from rag_system import NPTERAGSystem\n",
    "    \n",
    "    # Initialize the RAG system (this will connect to existing Qdrant collection)\n",
    "    # The NPTERAGSystem already uses the correct path \"./qdrant_data\" relative to backend\n",
    "    rag_system = NPTERAGSystem()\n",
    "    print(\"✅ RAG system initialized with existing Qdrant collection\")\n",
    "    rag_system.setup_collection()\n",
    "    \n",
    "    # Check if documents are already in the vector store\n",
    "    try:\n",
    "        # Try to retrieve some documents to verify the vector store is populated\n",
    "        test_docs = rag_system.retrieve_relevant_context(\"therapy\", k=5)\n",
    "        if test_docs:\n",
    "            print(f\"✅ Connected to existing vector store with {len(test_docs)} sample documents\")\n",
    "            print(\"Using pre-embedded documents from Qdrant collection\")\n",
    "            return rag_system\n",
    "        else:\n",
    "            print(\"⚠️ Vector store appears to be empty, but continuing...\")\n",
    "            return rag_system\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error checking vector store: {e}\")\n",
    "        print(\"Continuing with RAG system...\")\n",
    "        return rag_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e759d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Synthetic Data Generation\n",
    "# ============================================================================\n",
    "\n",
    "def create_knowledge_graph(docs, generator_llm, generator_embeddings):\n",
    "    \"\"\"Create knowledge graph from documents\"\"\"\n",
    "    from ragas.testset.graph import KnowledgeGraph, Node, NodeType\n",
    "    \n",
    "    print(\"Creating knowledge graph...\")\n",
    "    kg = KnowledgeGraph()\n",
    "    \n",
    "    # Use a subset of data for cost/time efficiency\n",
    "    for doc in docs[:20]:\n",
    "        kg.nodes.append(\n",
    "            Node(\n",
    "                type=NodeType.DOCUMENT,\n",
    "                properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(kg.nodes)} nodes to knowledge graph\")\n",
    "    return kg\n",
    "\n",
    "def apply_transformations(kg, docs, generator_llm, generator_embeddings):\n",
    "    \"\"\"Apply default transformations to knowledge graph\"\"\"\n",
    "    from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "    \n",
    "    print(\"Applying transformations...\")\n",
    "    transformer_llm = generator_llm\n",
    "    embedding_model = generator_embeddings\n",
    "    \n",
    "    default_transforms = default_transforms(documents=docs, llm=transformer_llm, embedding_model=embedding_model)\n",
    "    apply_transforms(kg, default_transforms)\n",
    "    \n",
    "    return kg\n",
    "\n",
    "def generate_synthetic_data(docs, generator_llm, generator_embeddings, testset_size=10):\n",
    "    \"\"\"Generate synthetic data using RAGAS\"\"\"\n",
    "    from ragas.testset import TestsetGenerator\n",
    "    \n",
    "    print(\"Generating synthetic data using RAGAS...\")\n",
    "    generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "    \n",
    "    # Use a smaller subset and smaller test size to avoid hanging\n",
    "    dataset = generator.generate_with_langchain_docs(docs[:5], testset_size=testset_size)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_basic_rag_chain(rag_system):\n",
    "    \"\"\"Set up RAG chain that matches the existing system for RAGAS evaluation\"\"\"\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_core.runnables import RunnablePassthrough\n",
    "    from langchain.schema import StrOutputParser\n",
    "    from operator import itemgetter\n",
    "    \n",
    "    print(\"Setting up RAG chain for evaluation...\")\n",
    "    \n",
    "    # Use the existing retriever from RAG system\n",
    "    retriever = rag_system.retriever\n",
    "    \n",
    "    # Create a prompt that matches your system's style\n",
    "    RAG_PROMPT = \"\"\"\\\n",
    "You are an NPTE-PT exam tutor assistant. Answer the question based ONLY on the provided context.\n",
    "\n",
    "If you cannot answer the question based on the context, say \"I don't have enough information to answer this question.\"\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)\n",
    "    \n",
    "    # Create LLM using the same model as your system\n",
    "    llm = ChatOpenAI(model=\"gpt-4.1-nano\")\n",
    "    \n",
    "    # Create RAG chain\n",
    "    rag_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "        | rag_prompt | llm | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f007c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_improved_rag_chain(rag_system):\n",
    "    \"\"\"Set up improved RAG chain with multi-query retriever - FIXED VERSION\"\"\"\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_core.runnables import RunnablePassthrough\n",
    "    from langchain.schema import StrOutputParser\n",
    "    from operator import itemgetter\n",
    "    from langchain.retrievers import MultiQueryRetriever\n",
    "    \n",
    "    print(\"Setting up improved RAG chain with multi-query retriever...\")\n",
    "    \n",
    "    # Use the existing retriever from RAG system as base\n",
    "    base_retriever = rag_system.retriever\n",
    "    \n",
    "    # Create LLM for query generation\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "    \n",
    "    # Create multi-query retriever\n",
    "    multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "        retriever=base_retriever,\n",
    "        llm=llm\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Multi-query retriever created successfully\")\n",
    "    \n",
    "    # Create an improved prompt\n",
    "    IMPROVED_RAG_PROMPT = \"\"\"\\\n",
    "You are an expert NPTE-PT exam tutor assistant. Answer the question based ONLY on the provided context.\n",
    "\n",
    "IMPORTANT INSTRUCTIONS:\n",
    "1. Use ONLY the information provided in the context below\n",
    "2. If the context doesn't contain enough information to answer the question, say \"I don't have enough information to answer this question based on the provided context.\"\n",
    "3. Do NOT add any information that is not in the context\n",
    "4. Be specific and direct in your answer\n",
    "5. If the context contains multiple relevant pieces of information, synthesize them clearly\n",
    "\n",
    "Context Information:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer (based only on the context above):\"\"\"\n",
    "\n",
    "    improved_prompt = ChatPromptTemplate.from_template(IMPROVED_RAG_PROMPT)\n",
    "    \n",
    "    # Create improved RAG chain with multi-query retriever\n",
    "    improved_rag_chain = (\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\") | multi_query_retriever,  # ✅ FIXED: using multi_query_retriever\n",
    "            \"question\": itemgetter(\"question\")\n",
    "        }\n",
    "        | improved_prompt | llm | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return improved_rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba466aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_evaluators():\n",
    "    \"\"\"Set up evaluation components with specific RAGAS metrics\"\"\"\n",
    "    from ragas.metrics import (\n",
    "        answer_relevancy,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness\n",
    "    )\n",
    "    from ragas.llms import LangchainLLMWrapper\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    print(\"Setting up RAGAS evaluators with updated metrics...\")\n",
    "    \n",
    "    # Create LLM for evaluation\n",
    "    eval_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "    \n",
    "    # RAGAS evaluators with the specific metrics you requested\n",
    "    evaluators = [\n",
    "        answer_relevancy,\n",
    "        context_precision,     # How precise the retrieved context is\n",
    "        context_recall,        # How much relevant context was retrieved\n",
    "        faithfulness          # How faithful the response is to the context\n",
    "    ]\n",
    "    \n",
    "    print(\"✅ RAGAS evaluators configured with:\")\n",
    "    print(\"   - response_relevancy: Measures how relevant the response is to the question\")\n",
    "    print(\"   - context_precision: Measures how precise the retrieved context is\")\n",
    "    print(\"   - context_recall: Measures how much relevant context was retrieved\")\n",
    "    print(\"   - faithfulness: Measures how faithful the response is to the context\")\n",
    "    \n",
    "    return evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3068148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_chain(rag_chain, dataset, evaluators):\n",
    "    \"\"\"Evaluate RAG chain using RAGAS 0.3.0 - FIXED VERSION\"\"\"\n",
    "    from ragas import evaluate\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from datasets import Dataset\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(\"Evaluating RAG chain with RAGAS 0.3.0...\")\n",
    "    \n",
    "    # Convert RAGAS TestsetSample to a proper evaluation dataset\n",
    "    df = dataset.to_pandas()\n",
    "    \n",
    "    # Create a new dataset with the required format for RAGAS evaluation\n",
    "    # We need to run the RAG chain on the questions to get answers\n",
    "    print(\"Running RAG chain on test questions...\")\n",
    "    \n",
    "    questions = []\n",
    "    ground_truths = []\n",
    "    contexts = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        question = row.get('user_input', row.get('question', ''))\n",
    "        ground_truth = row.get('reference', row.get('answer', ''))\n",
    "        context = row.get('reference_contexts', [])\n",
    "        \n",
    "        if question and ground_truth:\n",
    "            questions.append(question)\n",
    "            ground_truths.append(ground_truth)\n",
    "            contexts.append(context)\n",
    "    \n",
    "    # Run the RAG chain to get predictions\n",
    "    predictions = []\n",
    "    for question in questions:\n",
    "        try:\n",
    "            answer = rag_chain.invoke({\"question\": question})\n",
    "            predictions.append(answer)\n",
    "        except Exception as e:\n",
    "            print(f\"Error running RAG chain on question: {e}\")\n",
    "            predictions.append(\"Error generating answer\")\n",
    "    \n",
    "    # Create the evaluation dataset\n",
    "    eval_data = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": predictions,\n",
    "        \"ground_truth\": ground_truths,\n",
    "        \"contexts\": contexts\n",
    "    }\n",
    "    \n",
    "    eval_dataset = Dataset.from_dict(eval_data)\n",
    "    print(f\"Created evaluation dataset with {len(eval_dataset)} examples\")\n",
    "    \n",
    "    # For RAGAS 0.3.0, we need to pass the LLM and embeddings separately\n",
    "    eval_llm = ChatOpenAI(model=\"gpt-4.1-nano\")\n",
    "    \n",
    "    # The correct order for RAGAS 0.3.0 is: evaluate(dataset, metrics, llm, embeddings, ...)\n",
    "    result = evaluate(\n",
    "        eval_dataset,      # dataset comes first\n",
    "        evaluators,        # metrics come second\n",
    "        llm=eval_llm,      # llm parameter\n",
    "        # embeddings parameter is optional and will use default if not provided\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae23b153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_langsmith_dataset(dataset, dataset_name=\"NPTE Evaluation Data\"):\n",
    "    \"\"\"Create LangSmith dataset from RAGAS dataset - FIXED VERSION\"\"\"\n",
    "    from langsmith import Client\n",
    "    \n",
    "    client = Client()\n",
    "    \n",
    "    # Check if dataset already exists\n",
    "    try:\n",
    "        existing_datasets = client.list_datasets()\n",
    "        existing_dataset = None\n",
    "        \n",
    "        for ds in existing_datasets:\n",
    "            if ds.name == dataset_name:\n",
    "                existing_dataset = ds\n",
    "                break\n",
    "        \n",
    "        if existing_dataset:\n",
    "            print(f\"✅ Dataset '{dataset_name}' already exists, using existing dataset\")\n",
    "            langsmith_dataset = existing_dataset\n",
    "        else:\n",
    "            print(f\"Creating new LangSmith dataset: {dataset_name}\")\n",
    "            langsmith_dataset = client.create_dataset(\n",
    "                dataset_name=dataset_name,\n",
    "                description=\"NPTE Evaluation Data\"\n",
    "            )\n",
    "            print(f\"✅ Created new dataset with ID: {langsmith_dataset.id}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking/creating dataset: {e}\")\n",
    "        print(\"Creating new dataset anyway...\")\n",
    "        langsmith_dataset = client.create_dataset(\n",
    "            dataset_name=dataset_name,\n",
    "            description=\"NPTE Evaluation Data\"\n",
    "        )\n",
    "    \n",
    "    # First, let's check what columns are actually available\n",
    "    df = dataset.to_pandas()\n",
    "    print(f\"Available columns: {list(df.columns)}\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Check if dataset already has examples\n",
    "    try:\n",
    "        existing_examples = list(client.list_examples(dataset_id=langsmith_dataset.id))\n",
    "        if existing_examples:\n",
    "            print(f\"⚠️ Dataset already has {len(existing_examples)} examples\")\n",
    "            print(\"Skipping example creation to avoid duplicates\")\n",
    "            return langsmith_dataset\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not check existing examples: {e}\")\n",
    "    \n",
    "    # Add examples to dataset with proper error handling\n",
    "    examples_added = 0\n",
    "    for idx, data_row in df.iterrows():\n",
    "        try:\n",
    "            # Handle different possible column names (including RAGAS specific ones)\n",
    "            question = None\n",
    "            answer = None\n",
    "            context = None\n",
    "            \n",
    "            # Try different possible column names for question\n",
    "            for col in ['question', 'questions', 'query', 'queries', 'user_input']:\n",
    "                if col in data_row:\n",
    "                    question = data_row[col]\n",
    "                    break\n",
    "            \n",
    "            # Try different possible column names for answer\n",
    "            for col in ['answer', 'answers', 'response', 'responses', 'reference']:\n",
    "                if col in data_row:\n",
    "                    answer = data_row[col]\n",
    "                    break\n",
    "            \n",
    "            # Try different possible column names for context\n",
    "            for col in ['context', 'contexts', 'documents', 'docs', 'reference_contexts']:\n",
    "                if col in data_row:\n",
    "                    context = data_row[col]\n",
    "                    break\n",
    "            \n",
    "            # If we found the required fields, create the example\n",
    "            if question and answer:\n",
    "                client.create_example(\n",
    "                    inputs={\"question\": question},\n",
    "                    outputs={\"answer\": answer},\n",
    "                    metadata={\"context\": context} if context else {},\n",
    "                    dataset_id=langsmith_dataset.id\n",
    "                )\n",
    "                examples_added += 1\n",
    "                print(f\"✅ Added example {idx + 1}\")\n",
    "            else:\n",
    "                print(f\"⚠️ Skipping example {idx + 1} - missing required fields\")\n",
    "                print(f\"   Question found: {question is not None}\")\n",
    "                print(f\"   Answer found: {answer is not None}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error adding example {idx + 1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if examples_added > 0:\n",
    "        print(f\"✅ Successfully added {examples_added} examples to LangSmith dataset\")\n",
    "    else:\n",
    "        print(\"⚠️ No examples were added to the dataset\")\n",
    "    \n",
    "    return langsmith_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe87b771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Loading documents and RAG system...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rag_system:Collection npte_materials already exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG system initialized with existing Qdrant collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olb/NPTE_Oly/OLYM_PTE/backend/rag_system.py:64: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-qdrant package and should be used instead. To use it run `pip install -U :class:`~langchain-qdrant` and import as `from :class:`~langchain_qdrant import Qdrant``.\n",
      "  self.vector_store = Qdrant(\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:rag_system:Retrieved 5 relevant documents for query: therapy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to existing vector store with 5 sample documents\n",
      "Using pre-embedded documents from Qdrant collection\n",
      "✅ Using existing Qdrant collection: npte_materials\n",
      "✅ Using existing embeddings model: nomic-embed-text:latest\n",
      "✅ Vector store path: ../backend/qdrant_data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Step 1: Load documents and RAG system\n",
    "print(\"\\n1. Loading documents and RAG system...\")\n",
    "rag_system = load_documents()\n",
    "    \n",
    "    # Verify we're using existing embeddings\n",
    "print(f\"✅ Using existing Qdrant collection: {rag_system.collection_name}\")\n",
    "print(f\"✅ Using existing embeddings model: nomic-embed-text:latest\")\n",
    "print(f\"✅ Vector store path: ../backend/qdrant_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73b2d8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Setting up RAGAS components...\n",
      "✅ RAGAS components configured to use same models as RAG system\n",
      "   - LLM: qwen:latest\n",
      "   - Embeddings: nomic-embed-text:latest\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Set up RAGAS components\n",
    "print(\"\\n2. Setting up RAGAS components...\")\n",
    "generator_llm, generator_embeddings = setup_ragas_components()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21d0a1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:rag_system:Retrieved 20 relevant documents for query: therapy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Retrieving documents from existing vector store...\n",
      "Retrieved 20 documents from existing vector store\n",
      "✅ Successfully using existing embeddings with 20 documents\n",
      "✅ Saved 20 documents to docs_dump.json\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Retrieve documents from existing vector store\n",
    "print(\"\\n3. Retrieving documents from existing vector store...\")\n",
    "docs = rag_system.retrieve_relevant_context(\"therapy\", k=20)\n",
    "print(f\"Retrieved {len(docs)} documents from existing vector store\")\n",
    "\n",
    "if len(docs) == 0:\n",
    "    print(\"❌ No documents found in vector store. Please ensure documents are loaded.\")\n",
    "    print(\"💡 Try running: cd ../backend && python check_qdrant.py\")\n",
    "    sys.exit(0)\n",
    "\n",
    "print(f\"✅ Successfully using existing embeddings with {len(docs)} documents\")\n",
    "    \n",
    "    # Save documents to JSON file for inspection\n",
    "import json\n",
    "docs_as_dicts = [{\"content\": doc.page_content, \"metadata\": doc.metadata} for doc in docs]\n",
    "\n",
    "with open(\"docs_dump.json\", \"w\") as f:\n",
    "    json.dump(docs_as_dicts, f, indent=2)\n",
    "print(f\"✅ Saved {len(docs)} documents to docs_dump.json\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c76bcf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Creating knowledge graph...\n",
      "Creating knowledge graph...\n",
      "Added 20 nodes to knowledge graph\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 4: Create knowledge graph\n",
    "print(\"\\n4. Creating knowledge graph...\")\n",
    "kg = create_knowledge_graph(docs, generator_llm, generator_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "711da049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. applying transformations ...\n",
      "Applying transformations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bbf63daa84441c39221389e175c7028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371762d325e3455b8595a340eaa2fc9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289f3492af814686818cd02ed1a626dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283f743893234f6d82c7ca84abc05cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Knowledge graph ready for synthetic data generation\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Apply transformations (skipping for now due to performance issues)\n",
    "print(\"\\n5. applying transformations ...\")\n",
    "kg = apply_transformations(kg, docs, generator_llm, generator_embeddings)\n",
    "print(\"✅ Knowledge graph ready for synthetic data generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4241b81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Generating synthetic data...\n",
      "Generating synthetic data using RAGAS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5a2f45592f48ecae753b04dd202f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bbffc40b3744a009d8e27342480ecef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e546e9caf6054a33a6587cb0fc497de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af29d1121d7a4835be426b64c02933a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ragas.testset.synthesizers.multi_hop.abstract:found 20 clusters\n",
      "INFO:ragas.testset.synthesizers.multi_hop.specific:found 1 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3086ecae6cf468ab196e182e1108870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b37861ea76f4661a7d2ff75ba2901b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ragas.testset.synthesizers.multi_hop.abstract:found 20 clusters\n",
      "INFO:ragas.testset.synthesizers.multi_hop.specific:found 1 clusters\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8dd145ef9b04e899ebf371be10580f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated 3 synthetic test examples\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Generate synthetic data\n",
    "print(\"\\n6. Generating synthetic data...\")\n",
    "dataset = generate_synthetic_data(docs, generator_llm, generator_embeddings, testset_size=2)\n",
    "print(f\"✅ Generated {len(dataset.to_pandas())} synthetic test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74f51019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. Creating LangSmith dataset...\n",
      "Creating new LangSmith dataset: NPTE Evaluation Data\n",
      "✅ Created new dataset with ID: f4739ba4-c31a-43b0-8928-601290f7684e\n",
      "Available columns: ['user_input', 'reference_contexts', 'reference', 'synthesizer_name']\n",
      "Dataset shape: (3, 4)\n",
      "✅ Added example 1\n",
      "✅ Added example 2\n",
      "✅ Added example 3\n",
      "✅ Successfully added 3 examples to LangSmith dataset\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Create LangSmith dataset\n",
    "print(\"\\n7. Creating LangSmith dataset...\")\n",
    "dataset_name = \"NPTE Evaluation Data\"\n",
    "langsmith_dataset = create_langsmith_dataset(dataset, dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "948881db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. Setting up RAG chains...\n",
      "Setting up RAG chain for evaluation...\n",
      "Setting up improved RAG chain for evaluation...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 8: Set up RAG chains\n",
    "print(\"\\n8. Setting up RAG chains...\")\n",
    "basic_rag_chain = setup_basic_rag_chain(rag_system)\n",
    "improved_rag_chain = setup_improved_rag_chain(rag_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa310cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9. Setting up evaluators...\n",
      "Setting up RAGAS evaluators with updated metrics...\n",
      "✅ RAGAS evaluators configured with:\n",
      "   - response_relevancy: Measures how relevant the response is to the question\n",
      "   - context_precision: Measures how precise the retrieved context is\n",
      "   - context_recall: Measures how much relevant context was retrieved\n",
      "   - faithfulness: Measures how faithful the response is to the context\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Set up evaluators\n",
    "print(\"\\n9. Setting up evaluators...\")\n",
    "evaluators = setup_evaluators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79938ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10. Evaluating basic RAG chain...\n",
      "Evaluating RAG chain with RAGAS 0.3.0...\n",
      "Running RAG chain on test questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created evaluation dataset with 3 examples\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'property' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Step 10: Evaluate basic chain\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m10. Evaluating basic RAG chain...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m basic_results = \u001b[43mevaluate_rag_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasic_rag_chain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mevaluate_rag_chain\u001b[39m\u001b[34m(rag_chain, dataset, evaluators)\u001b[39m\n\u001b[32m     53\u001b[39m eval_llm = ChatOpenAI(model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4.1-nano\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# The correct order for RAGAS 0.3.0 is: evaluate(dataset, metrics, llm, embeddings, ...)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m result = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# dataset comes first\u001b[39;49;00m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# metrics come second\u001b[39;49;00m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_llm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# llm parameter\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# embeddings parameter is optional and will use default if not provided\u001b[39;49;00m\n\u001b[32m     61\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NPTE_Oly/OLYM_PTE/backend/.venv/lib/python3.13/site-packages/ragas/_analytics.py:227\u001b[39m, in \u001b[36mtrack_was_completed.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> t.Any:\n\u001b[32m    226\u001b[39m     track(IsCompleteEvent(event_type=func.\u001b[34m__name__\u001b[39m, is_completed=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m     track(IsCompleteEvent(event_type=func.\u001b[34m__name__\u001b[39m, is_completed=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NPTE_Oly/OLYM_PTE/backend/.venv/lib/python3.13/site-packages/ragas/evaluation.py:176\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar)\u001b[39m\n\u001b[32m    173\u001b[39m     dataset = EvaluationDataset.from_list(dataset.to_list())\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, EvaluationDataset):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[43mvalidate_required_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m     validate_supported_metrics(dataset, metrics)\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# set the llm and embeddings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NPTE_Oly/OLYM_PTE/backend/.venv/lib/python3.13/site-packages/ragas/validation.py:60\u001b[39m, in \u001b[36mvalidate_required_columns\u001b[39m\u001b[34m(ds, metrics)\u001b[39m\n\u001b[32m     58\u001b[39m metric_type = get_supported_metric_type(ds)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     required_columns = \u001b[38;5;28mset\u001b[39m(\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequired_columns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(metric_type, []))\n\u001b[32m     61\u001b[39m     available_columns = \u001b[38;5;28mset\u001b[39m(ds.features())\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m required_columns.issubset(available_columns):\n",
      "\u001b[31mAttributeError\u001b[39m: 'property' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# Step 10: Evaluate basic chain\n",
    "print(\"\\n10. Evaluating basic RAG chain...\")\n",
    "basic_results = evaluate_rag_chain(basic_rag_chain, dataset, evaluators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a59cc866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11. Evaluating improved RAG chain...\n",
      "Evaluating RAG chain with RAGAS 0.3.0...\n",
      "Running RAG chain on test questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created evaluation dataset with 3 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a0e2dc1de946ccba6a3e8cb7736918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.473535 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Evaluate improved chain\n",
    "print(\"\\n11. Evaluating improved RAG chain...\")\n",
    "improved_results = evaluate_rag_chain(improved_rag_chain, dataset, evaluators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a44365d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5fffd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "Basic RAG Chain Results:\n",
      "{'answer_relevancy': 0.0000, 'faithfulness': 0.0833, 'context_recall': 1.0000, 'context_precision': 0.8333}\n",
      "\n",
      "Improved RAG Chain Results:\n",
      "{'answer_relevancy': 0.0000, 'faithfulness': 0.0000, 'context_recall': 1.0000, 'context_precision': 0.8333}\n",
      "\n",
      "============================================================\n",
      "EVALUATION COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# Step 12: Display results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Basic RAG Chain Results:\")\n",
    "print(basic_results)\n",
    "print(\"\\nImproved RAG Chain Results:\")\n",
    "print(improved_results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION COMPLETE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
